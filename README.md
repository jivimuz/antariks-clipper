# Antariks Clipper

Auto-generate viral highlight clips from YouTube videos or manual uploads. Output vertical 9:16 format ready for Reels/TikTok.

## Features

- ğŸ¬ **Multiple Input Sources**: YouTube URL or manual video upload
- ğŸ¤– **AI-Powered Highlights**: Automatic clip generation with smart scoring
- ğŸ“ **Transcription**: Faster-whisper for accurate speech-to-text
- ğŸ¯ **Face Tracking**: Active speaker detection and auto-reframe (optional)
- ğŸ“± **Vertical Output**: 1080x1920 (9:16) format for social media
- ğŸ’¬ **Captions**: Burn-in subtitle support (optional)
- ğŸš€ **Simple Setup**: No Docker, Redis, or complex dependencies

## Quick Start

### Prerequisites

1. **Python 3.8+** (for backend)
2. **Node.js 18+** (for frontend)
3. **FFmpeg** - Must be installed on your system:
   - **Windows**: Download from https://ffmpeg.org/download.html and add to PATH
   - **macOS**: `brew install ffmpeg`
   - **Linux**: `sudo apt-get install ffmpeg` (Ubuntu/Debian) or `sudo yum install ffmpeg` (CentOS/RHEL)

### Backend Setup

```bash
cd backend

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run server
uvicorn app:app --reload --port 8000
```

Backend will be available at http://localhost:8000

### Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Run development server
npm run dev
```

Frontend will be available at http://localhost:3000

### Usage

1. Open http://localhost:3000
2. Choose YouTube URL or Upload video
3. Submit and wait for processing
4. View generated highlight clips
5. Render clips with optional face tracking or captions
6. Download vertical 9:16 videos ready for social media

## Technology Stack

### Backend
- **FastAPI** - Modern Python web framework
- **SQLite** - Simple database (no ORM)
- **yt-dlp** - YouTube video download
- **faster-whisper** - Speech-to-text transcription
- **OpenCV + MediaPipe** - Face detection and tracking
- **FFmpeg** - Video processing via subprocess
- **ThreadPoolExecutor** - Background job processing (no external queue)

### Frontend
- **Next.js 15** - React framework with App Router
- **TypeScript** - Type-safe development
- **Tailwind CSS** - Utility-first styling

## Project Structure

```
/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py              # FastAPI application
â”‚   â”œâ”€â”€ db.py               # SQLite database operations
â”‚   â”œâ”€â”€ config.py           # Configuration
â”‚   â”œâ”€â”€ services/           # Business logic modules
â”‚   â”‚   â”œâ”€â”€ downloader.py   # YouTube/upload handling
â”‚   â”‚   â”œâ”€â”€ ffmpeg.py       # Video processing utilities
â”‚   â”‚   â”œâ”€â”€ transcribe.py   # Whisper transcription
â”‚   â”‚   â”œâ”€â”€ highlight.py    # Clip generation & scoring
â”‚   â”‚   â”œâ”€â”€ thumbnails.py   # Thumbnail extraction
â”‚   â”‚   â”œâ”€â”€ face_track.py   # MediaPipe face detection
â”‚   â”‚   â”œâ”€â”€ reframe.py      # Active speaker tracking
â”‚   â”‚   â”œâ”€â”€ render.py       # Vertical video rendering
â”‚   â”‚   â””â”€â”€ jobs.py         # Background job processing
â”‚   â”œâ”€â”€ data/               # Generated files (gitignored)
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx        # Home page (input)
â”‚   â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx    # Jobs list
â”‚   â”‚   â”‚   â””â”€â”€ [id]/page.tsx  # Job detail with clips
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ README.md
â””â”€â”€ README.md               # This file
```

## API Endpoints

### Jobs
- `POST /api/jobs` - Create job (YouTube or upload)
- `GET /api/jobs` - List all jobs
- `GET /api/jobs/{job_id}` - Get job details
- `GET /api/jobs/{job_id}/clips` - Get job clips

### Renders
- `POST /api/clips/{clip_id}/render` - Create render job
- `GET /api/renders/{render_id}` - Get render status
- `GET /api/renders/{render_id}/download` - Download video

### Assets
- `GET /api/thumbnails/{clip_id}` - Get clip thumbnail

## Processing Pipeline

1. **Acquire**: Download (YouTube) or save upload
2. **Normalize**: Convert to standard format (H.264/AAC)
3. **Transcribe**: Generate transcript with word-level timestamps
4. **Generate Highlights**: Score segments based on:
   - Hook keywords (Indonesian + English)
   - Word density (unique words ratio)
   - Duration preference (ideal ~35 seconds)
5. **Create Thumbnails**: Extract mid-frame for each clip
6. **Render**: Create vertical 9:16 output with optional:
   - Face tracking (active speaker detection)
   - Captions (burned-in subtitles)

## Notes

- First run downloads Whisper model (~150MB for base model)
- Processing time depends on video length and options
- Face tracking is slower but provides better framing for podcasts
- All data stored locally in `backend/data/`
- No external services or API keys required

## License

MIT
